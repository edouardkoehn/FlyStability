{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook that generates the plot for the training of random model with fixed amount of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flybrain.utils as utils\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiments(nSamples:int,\n",
    "                     subfolder:str, \n",
    "                     activation:str, \n",
    "                     weights=True, \n",
    "                     shifts=False, \n",
    "                     gains=False,\n",
    "                     N=100,\n",
    "                     Nle=1,\n",
    "                     Epochs=400,\n",
    "                     lr=0.1 ,\n",
    "                     loss='MSE',\n",
    "                     target='0.00',\n",
    "                     g=1.0,\n",
    "                     tons=0.2,\n",
    "                     tsim=200,\n",
    "                     dt=0.1, \n",
    "                     param=\"0_100_0\"):\n",
    "    data=[]\n",
    "    gen_path=os.path.join(utils.get_root(), 'data', 'logs',subfolder) \n",
    "    if loss=='Entropy':\n",
    "        model_path=f\"{activation}_Weights{weights}_Shifts{shifts}_Gains{gains}_N{N}_lr{lr}_NLE{Nle}_Epochs{Epochs}_{loss}_g{g}_Tons{tons}_Tsim{tsim}_Param{param}\"\n",
    "    \n",
    "    else:\n",
    "        model_path=f\"{activation}_Weights{weights}_Shifts{shifts}_Gains{gains}_N{N}_lr{lr}_NLE{Nle}_Epochs{Epochs}_{loss}_{target}_g{g}_Tons{tons}_Tsim{tsim}_Param{param}\"\n",
    "    for i in range(nSamples):\n",
    "        path=os.path.join(gen_path, model_path +f\"_Sample{i}_logs.json\")\n",
    "        data.append(utils.load_logs(file_path=path))\n",
    "    return data\n",
    "\n",
    "def load_experiments_weights(nSamples:int,\n",
    "                     subfolder:str, \n",
    "                     activation:str, \n",
    "                     weights=True, \n",
    "                     shifts=False, \n",
    "                     gains=False,\n",
    "                     N=100,\n",
    "                     Nle=1,\n",
    "                     Epochs=400,\n",
    "                     lr=0.01 ,\n",
    "                     loss='MSE',\n",
    "                     target='0.00',\n",
    "                     g=1.0,\n",
    "                     tons=0.2,\n",
    "                     tsim=200,\n",
    "                     dt=0.1):\n",
    "    data=[]\n",
    "    gen_path=os.path.join(utils.get_root(), 'data', 'logs',subfolder,'weigth') \n",
    "    if loss=='Entropy':\n",
    "        model_path=f\"{activation}_Weights{weights}_Shifts{shifts}_Gains{gains}_N{N}_lr{lr}_NLE{Nle}_Epochs{Epochs}_{loss}_g{g}_Tons{tons}_Tsim{tsim}_dt{dt}\"\n",
    "    \n",
    "    else:\n",
    "        model_path=f\"{activation}_Weights{weights}_Shifts{shifts}_Gains{gains}_N{N}_lr{lr}_NLE{Nle}_Epochs{Epochs}_{loss}_{target}_g{g}_Tons{tons}_Tsim{tsim}_dt{dt}\"\n",
    "    for i in range(nSamples):\n",
    "        path=os.path.join(gen_path, model_path +f\"_Sample{i}_logs.json\")\n",
    "        data.append(utils.load_logs(file_path=path))\n",
    "    return data\n",
    "\n",
    "def load_experiments_activation(nSamples:int,\n",
    "                     subfolder:str, \n",
    "                     activation:str, \n",
    "                     weights=False, \n",
    "                     shifts=True, \n",
    "                     gains=True,\n",
    "                     N=100,\n",
    "                     Nle=1,\n",
    "                     Epochs=400,\n",
    "                     lr=0.1 ,\n",
    "                     loss='MSE',\n",
    "                     target='0.00',\n",
    "                     g=1.0,\n",
    "                     tons=0.2,\n",
    "                     tsim=200,\n",
    "                     dt=0.1):\n",
    "    data=[]\n",
    "    gen_path=os.path.join(utils.get_root(), 'data', 'logs',subfolder,'activation') \n",
    "    if loss=='Entropy':\n",
    "        model_path=f\"{activation}_Weights{weights}_Shifts{shifts}_Gains{gains}_N{N}_lr{lr}_NLE{Nle}_Epochs{Epochs}_{loss}_g{g}_Tons{tons}_Tsim{tsim}_dt{dt}\"\n",
    "    \n",
    "    else:\n",
    "        model_path=f\"{activation}_Weights{weights}_Shifts{shifts}_Gains{gains}_N{N}_lr{lr}_NLE{Nle}_Epochs{Epochs}_{loss}_{target}_g{g}_Tons{tons}_Tsim{tsim}_dt{dt}\"\n",
    "    for i in range(nSamples):\n",
    "        path=os.path.join(gen_path, model_path +f\"_Sample{i}_logs.json\")\n",
    "        data.append(utils.load_logs(file_path=path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Load the runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanhpos_n100_nle1_weights_N2=load_experiments_weights(nSamples=5,subfolder='rd_rnn',activation='tanh_positive', Nle=1)\n",
    "tanhpos_n100_nle10_weights_N2=load_experiments_weights(nSamples=5,subfolder='rd_rnn',activation='tanh_positive', Nle=10)\n",
    "tanhpos_n100_nle25_weights_N2=load_experiments_weights(nSamples=5,subfolder='rd_rnn',activation='tanh_positive', Nle=25)\n",
    "tanhpos_n100_nle50_weights_N2=load_experiments_weights(nSamples=5,subfolder='rd_rnn',activation='tanh_positive', Nle=50)\n",
    "tanhpos_n100_nle75_weights_N2=load_experiments_weights(nSamples=5,subfolder='rd_rnn',activation='tanh_positive', Nle=75)\n",
    "tanhpos_n100_weights_N2=[tanhpos_n100_nle1_weights_N2,\n",
    "         tanhpos_n100_nle10_weights_N2,\n",
    "         tanhpos_n100_nle25_weights_N2,\n",
    "         tanhpos_n100_nle50_weights_N2,\n",
    "         tanhpos_n100_nle75_weights_N2,]\n",
    "\n",
    "tanhpos_n100_nle1_weights_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=1, weights=True,gains=False,shifts=False, param=\"100_0_0\")\n",
    "tanhpos_n100_nle10_weights_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=10, weights=True,gains=False,shifts=False, param=\"100_0_0\")\n",
    "tanhpos_n100_nle25_weights_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=25, weights=True,gains=False,shifts=False, param=\"100_0_0\")\n",
    "tanhpos_n100_nle50_weights_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=50, weights=True,gains=False,shifts=False, param=\"100_0_0\")\n",
    "tanhpos_n100_nle75_weights_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=75, weights=True,gains=False,shifts=False, param=\"100_0_0\")\n",
    "tanhpos_n100_weights_N=[tanhpos_n100_nle1_weights_N,\n",
    "                    tanhpos_n100_nle10_weights_N,\n",
    "                    tanhpos_n100_nle25_weights_N,\n",
    "                    tanhpos_n100_nle50_weights_N,\n",
    "                    tanhpos_n100_nle75_weights_N]\n",
    "\n",
    "tanhpos_n100_nle1_gains_shifts_2N=load_experiments_activation(nSamples=5,subfolder='rd_rnn',activation='tanh_positive', Nle=1)\n",
    "tanhpos_n100_nle10_gains_shifts_2N=load_experiments_activation(nSamples=5,subfolder='rd_rnn',activation='tanh_positive', Nle=10)\n",
    "tanhpos_n100_nle25_gains_shifts_2N=load_experiments_activation(nSamples=5,subfolder='rd_rnn',activation='tanh_positive', Nle=25)\n",
    "tanhpos_n100_nle50_gains_shifts_2N=load_experiments_activation(nSamples=5,subfolder='rd_rnn',activation='tanh_positive', Nle=50)\n",
    "tanhpos_n100_nle75_gains_shifts_2N=load_experiments_activation(nSamples=5,subfolder='rd_rnn',activation='tanh_positive', Nle=75)\n",
    "tanhpos_n100_gains_shifts_2N=[tanhpos_n100_nle1_gains_shifts_2N,\n",
    "         tanhpos_n100_nle10_gains_shifts_2N,\n",
    "         tanhpos_n100_nle25_gains_shifts_2N,\n",
    "         tanhpos_n100_nle50_gains_shifts_2N,\n",
    "         tanhpos_n100_nle75_gains_shifts_2N]\n",
    "\n",
    "tanhpos_n100_nle1_gains_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=1, weights=False,gains=True,shifts=False, param=\"0_100_0\")\n",
    "tanhpos_n100_nle10_gains_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=10, weights=False,gains=True,shifts=False, param=\"0_100_0\")\n",
    "tanhpos_n100_nle25_gains_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=25, weights=False,gains=True,shifts=False, param=\"0_100_0\")\n",
    "tanhpos_n100_nle50_gains_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=50, weights=False,gains=True,shifts=False, param=\"0_100_0\")\n",
    "tanhpos_n100_nle75_gains_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=75, weights=False,gains=True,shifts=False, param=\"0_100_0\")\n",
    "tanhpos_n100_gains_N=[tanhpos_n100_nle1_gains_N,\n",
    "                    tanhpos_n100_nle10_gains_N,\n",
    "                    tanhpos_n100_nle25_gains_N,\n",
    "                    tanhpos_n100_nle50_gains_N,\n",
    "                    tanhpos_n100_nle75_gains_N]\n",
    "\n",
    "\n",
    "tanhpos_n100_nle1_shifts_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=1, weights=False,gains=False,shifts=True, param=\"0_0_100\")\n",
    "tanhpos_n100_nle10_shifts_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=10, weights=False,gains=False,shifts=True, param=\"0_0_100\")\n",
    "tanhpos_n100_nle25_shifts_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=25, weights=False,gains=False,shifts=True, param=\"0_0_100\")\n",
    "tanhpos_n100_nle50_shifts_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=50, weights=False,gains=False,shifts=True, param=\"0_0_100\")\n",
    "tanhpos_n100_nle75_shifts_N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=75, weights=False,gains=False,shifts=True, param=\"0_0_100\")\n",
    "tanhpos_n100_shifts_N=[tanhpos_n100_nle1_shifts_N,\n",
    "                    tanhpos_n100_nle10_shifts_N,\n",
    "                    tanhpos_n100_nle25_shifts_N,\n",
    "                    tanhpos_n100_nle50_shifts_N,\n",
    "                    tanhpos_n100_nle75_shifts_N]\n",
    "\n",
    "tanhpos_n100_nle1_weights_2N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=1, weights=True,gains=False,shifts=False, param=\"200_0_0\")\n",
    "tanhpos_n100_nle10_weights_2N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=10, weights=True,gains=False,shifts=False, param=\"200_0_0\")\n",
    "tanhpos_n100_nle25_weights_2N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=25, weights=True,gains=False,shifts=False, param=\"200_0_0\")\n",
    "tanhpos_n100_nle50_weights_2N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=50, weights=True,gains=False,shifts=False, param=\"200_0_0\")\n",
    "tanhpos_n100_nle75_weights_2N=load_experiments(nSamples=3,subfolder='rd_RNN_fixed_param',activation='tanh_positive', Nle=75, weights=True,gains=False,shifts=False, param=\"200_0_0\")\n",
    "tanhpos_n100_weights_2N=[tanhpos_n100_nle1_weights_2N,\n",
    "                         tanhpos_n100_nle10_weights_2N,\n",
    "                    tanhpos_n100_nle25_weights_2N,\n",
    "                    tanhpos_n100_nle50_weights_2N,\n",
    "                    tanhpos_n100_nle75_weights_2N]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Plot the training logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logs_trainings(list_of_logs, \n",
    "                        entry_used, \n",
    "                        axs,\n",
    "                        serie, \n",
    "                        color):\n",
    "    data=np.zeros((len(list_of_logs),len(list_of_logs[0][entry_used])))\n",
    "    for i, sample in enumerate(list_of_logs):\n",
    "        run = np.array(sample[entry_used])\n",
    "        axs.plot(np.arange(len(run)),run, alpha=0.2, color=color, lw=1)\n",
    "    \n",
    "    # Assign the processed run to the data matrix\n",
    "        data[i, :] = run\n",
    "    mean=np.nanmean(data[:,:-1], axis=0 )\n",
    "    var=np.nanstd(data[:,:-1], axis=0)\n",
    "    epochs=np.arange(len(mean))\n",
    "    \n",
    "    axs.plot(epochs, mean,label=f'{serie}' ,alpha=0.9, lw=2,color=color)\n",
    "    #axs.fill_between(epochs,mean-var,mean+var,alpha=0.1\n",
    "                    #marker=\"D\",\n",
    "                    #markersize=1,\n",
    "                    #linewidth=0.5,label=fr'$T_{\"on\"}:{tON}$'\n",
    "    #                )\n",
    "    axs.legend()\n",
    "    return\n",
    "\n",
    "def plot_logs_spectrums(list_of_logs, \n",
    "                        entry_used, \n",
    "                        axs,\n",
    "                        serie, \n",
    "                        color):\n",
    "    data=np.zeros((len(list_of_logs),len(list_of_logs[0][entry_used])))\n",
    "    for i, sample in enumerate(list_of_logs):\n",
    "        run = np.array(sample[entry_used])\n",
    "        axs.plot(np.linspace(0,100, len(run)),run, alpha=0.2, color=color, lw=1)\n",
    "    \n",
    "    # Assign the processed run to the data matrix\n",
    "        data[i, :] = run\n",
    "    mean=np.nanmean(data[:,:-1], axis=0 )\n",
    "    var=np.nanstd(data[:,:-1], axis=0)\n",
    "    epochs=np.linspace(0,100, len(mean))\n",
    "    \n",
    "    axs.plot(epochs, mean,label=f'{serie}' ,alpha=0.9, lw=2,color=color)\n",
    "    #axs.fill_between(epochs,mean-var,mean+var,alpha=0.1\n",
    "                    #marker=\"D\",\n",
    "                    #markersize=1,\n",
    "                    #linewidth=0.5,label=fr'$T_{\"on\"}:{tON}$'\n",
    "    #                )\n",
    "    axs.legend()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3) Plot the convergence as function of the number of lyapunov exponent used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conv_grad_based(list_of_logs, window_size=50):\n",
    "    grad_weigths=np.zeros((len(list_of_logs),len(list_of_logs[0]['grad_weights'])))\n",
    "    grad_shifts=np.zeros((len(list_of_logs),len(list_of_logs[0]['grad_shifts'])))\n",
    "    grad_gains=np.zeros((len(list_of_logs),len(list_of_logs[0]['grad_gains'])))\n",
    "\n",
    "    for i, sample in enumerate(list_of_logs):\n",
    "        grad_weigths[i, :]=np.array(sample['grad_weights'])\n",
    "        grad_shifts[i, :]=np.array(sample['grad_shifts'])\n",
    "        grad_gains[i, :]=np.array(sample['grad_gains'])\n",
    "        #plt.plot(np.arange(100),grad_shifts[i, -100:])\n",
    "\n",
    "    mean_weight=np.mean(grad_weigths[:, -window_size:], axis=1)\n",
    "    mean_shifts=np.mean(grad_shifts[:, -window_size:], axis=1)\n",
    "    mean_gains=np.mean(grad_gains[:, -window_size:], axis=1)\n",
    "    \n",
    "    return {'weights':mean_weight,\n",
    "            'gains':mean_gains,\n",
    "            'shifts':mean_shifts}\n",
    "    \n",
    "def compute_conv_loss_based(list_of_logs, window_size=50):\n",
    "    loss=np.zeros((len(list_of_logs),len(list_of_logs[0]['training_loss'])))\n",
    "\n",
    "    for i, sample in enumerate(list_of_logs):\n",
    "        loss[i, :]=np.array(sample['training_loss'])\n",
    "        #plt.plot(np.arange(100),grad_shifts[i, -100:])\n",
    "\n",
    "    mean_loss=np.mean(loss[:, -window_size:], axis=1)\n",
    "    return {'loss':mean_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conv_grad_based(expriment_lists, experiment_names,ax, key_list=['weights'], lab=''):\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for exp, label in zip(expriment_lists, experiment_names):\n",
    "        dict_conv =compute_conv_grad_based(exp)\n",
    "        combined_values=0\n",
    "        for key in key_list:\n",
    "            combined_values += dict_conv[key]   # Combine gains and shifts\n",
    "        for value in combined_values:\n",
    "            data.append({'Experiment': label, 'Value': value})\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "  \n",
    "    # Calculate means and error bars (standard deviation or standard error)\n",
    "    summary = df.groupby('Experiment')['Value'].agg(['mean', 'std']).reset_index()\n",
    "    summary['x'] = range(1, len(summary) + 1)\n",
    "    # Plot using matplotlib\n",
    "    \n",
    "    #ax.scatter(df['Experiment'], df['Value'], color='black', alpha=0.2)\n",
    "    #ax.scatter(summary['Experiment'], summary['mean'], color='black', marker='d',alpha=0.8)\n",
    "    #ax.errorbar(summary['Experiment'], summary['mean'], yerr=summary['std'], fmt='o', markersize=10, lw=1,capsize=4, alpha=0.4, label=lab)\n",
    "    ax.plot(summary['Experiment'], summary['mean'],marker='o', lw=2, alpha=0.8, label=lab)\n",
    "    # Customize the plot\n",
    "    ax.spines[\"right\"].set_color(\"none\")\n",
    "    ax.spines[\"top\"].set_color(\"none\")\n",
    "    ax.set_xticks(summary['Experiment'])\n",
    "    ax.set_xticklabels(summary['Experiment'], fontsize=12)\n",
    "    \n",
    "def plot_conv_loss_based(expriment_lists, experiment_names,ax, lab='', color='blue', ls='-', marker='o'):\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for exp, label in zip(expriment_lists, experiment_names):\n",
    "        dict_conv =compute_conv_loss_based(exp)\n",
    "        combined_values=dict_conv['loss']\n",
    "        for value in combined_values:\n",
    "            data.append({'Experiment': label, 'Value': value})\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "  \n",
    "    # Calculate means and error bars (standard deviation or standard error)\n",
    "    summary = df.groupby('Experiment')['Value'].agg(['mean', 'std']).reset_index()\n",
    "    summary['x'] = range(1, len(summary) + 1)\n",
    "    # Plot using matplotlib\n",
    "    \n",
    "    #ax.scatter(df['Experiment'], df['Value'], color='black', alpha=0.2)\n",
    "    #ax.scatter(summary['Experiment'], summary['mean'], color='black', marker='d',alpha=0.8)\n",
    "    #ax.errorbar(summary['Experiment'], summary['mean'], yerr=summary['std'], fmt='o', markersize=10, lw=1,capsize=4, alpha=0.4, label=lab)\n",
    "    ax.plot(summary['Experiment'], summary['mean'],marker=marker,markersize=10, lw=2, ls=ls,alpha=0.8, label=lab)\n",
    "    # Customize the plot\n",
    "    ax.spines[\"right\"].set_color(\"none\")\n",
    "    ax.spines[\"top\"].set_color(\"none\")\n",
    "    ax.set_xticks(summary['Experiment'])\n",
    "    ax.set_xticklabels(summary['Experiment'], fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=7\n",
    "b=20\n",
    "b/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=5\n",
    "fig, ax=plt.subplots(1,1, figsize=(int(a*2.857),a),sharex=True)\n",
    "ax.spines[\"right\"].set_color(\"none\")\n",
    "ax.spines[\"top\"].set_color(\"none\")\n",
    "\n",
    "experiment_labels = [1, 10, 25, 50, 75, 100]\n",
    "plot_conv_loss_based(tanhpos_n100_weights_N,experiment_labels,ax,lab=r'$N$ parameters, Weights', marker='D')\n",
    "plot_conv_loss_based(tanhpos_n100_gains_N,experiment_labels,ax,lab=r'$N$ parameters,Gains', marker='D')\n",
    "#plot_conv_loss_based(tanhpos_n100_shifts_N,experiment_labels,ax,lab=r'$N$ parameters,Shifts',marker='D')\n",
    "plot_conv_loss_based(tanhpos_n100_weights_2N,experiment_labels,ax,lab=r'$2N$ parameters, Weights', marker='P')\n",
    "plot_conv_loss_based(tanhpos_n100_gains_shifts_2N,experiment_labels,ax,lab=r'$2N$ parameters, Gains and shifts', marker='P')\n",
    "plot_conv_loss_based(tanhpos_n100_weights_N2,experiment_labels,ax,lab= \"$N(N-1)$ parameters,Weights \",  marker='^')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plot_conv_loss_based(tanhpos_n100_shifts_N,experiment_labels,ax,lab=r'$N$ parameters,Shifts')\n",
    "\n",
    "\n",
    "ax.set_ylabel(r\"$\\frac{1}{N} \\sum_{T-50}^{T}L_{\\theta}^t$\", fontsize=24)\n",
    "ax.set_xlabel(\"Number of Lyapunov exponent\", fontsize=24)\n",
    "ax.tick_params(axis='both', which='major', labelsize=22)\n",
    "#ax.set_yscale('log')\n",
    "#ax.set_ylim([1E-15, 1E1])\n",
    "\n",
    "ax.legend(fontsize=22, frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/fig/FINAL/3_RD_Fixed_convergence_loss_based.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(1,1, figsize=(7,5),sharex=True)\n",
    "ax.spines[\"right\"].set_color(\"none\")\n",
    "ax.spines[\"top\"].set_color(\"none\")\n",
    "\n",
    "experiment_labels = [1, 10, 25, 50, 75, 100]\n",
    "plot_conv_grad_based(tanhpos_n100_weights_N2,experiment_labels,ax,key_list=['weights'],lab= \"Weights, $N^2$ parameters\")\n",
    "plot_conv_grad_based(tanhpos_n100_gains_shifts_2N,experiment_labels,ax,key_list=['gains','shifts'] ,lab=r'Gains and shifts, $2N$ parameters')\n",
    "plot_conv_grad_based(tanhpos_n100_weights_2N,experiment_labels,ax,key_list=['weights'],lab=r'Weights, $2N$ parameters')\n",
    "plot_conv_grad_based(tanhpos_n100_weights_N,experiment_labels,ax,key_list=['weights'],lab=r'Weights, $N$ parameters')\n",
    "#plot_conv_grad_based(tanhpos_n100_gains_N,experiment_labels,ax,key_list=['gains'],lab=r'Gains, $N$ parameters')\n",
    "#plot_conv_grad_based(tanhpos_n100_shifts_N,experiment_labels,ax,key_list=['shifts'],lab=r'Shifts, $N$ parameters')\n",
    "\n",
    "\n",
    "ax.set_ylabel(r\"$|\\nabla L_{\\theta}|$\", fontsize=20)\n",
    "ax.set_xlabel(\"Number of lyapunov exponent\", fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "#ax.set_yscale('log')\n",
    "#ax.set_ylim([1E-15, 1E1])\n",
    "#ax.set_xlim([0,90])\n",
    "ax.legend(fontsize=10, frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/fig/FINAL/3_RD_Fixed_convergence_grad_based.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Plot the time of compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time(expriment_lists, experiment_names,ax, lab=''):\n",
    "    # Prepare data for plotting\n",
    "    data = []\n",
    "    for exp, label in zip(expriment_lists, experiment_names):\n",
    "        time=[]\n",
    "        for run in exp:\n",
    "            time.append(float(run['time_training'])/400)\n",
    "        combined_values = time\n",
    "        for value in combined_values:\n",
    "            data.append({'Experiment': label, 'Value': value})\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Calculate means and error bars (standard deviation or standard error)\n",
    "    summary = df.groupby('Experiment')['Value'].agg(['mean', 'std']).reset_index()\n",
    "    summary['x'] = range(1, len(summary) + 1)\n",
    "    # Plot using matplotlib\n",
    "\n",
    "    #ax.scatter(df['Experiment'], df['Value'], alpha=0.2)\n",
    "    ax.plot(summary['Experiment'], summary['mean'],marker='o',alpha=0.7, label=lab)\n",
    "    #ax.errorbar(summary['Experiment'], summary['mean'], yerr=summary['std'], fmt='o', markersize=10, lw=1,capsize=4, label='Error (std)', alpha=0.4)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.spines[\"right\"].set_color(\"none\")\n",
    "    ax.spines[\"top\"].set_color(\"none\")\n",
    "    ax.set_xticks(summary['Experiment'])\n",
    "    ax.set_xticklabels(summary['Experiment'], fontsize=12)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(1,1, figsize=(7,5),sharex=True)\n",
    "ax.spines[\"right\"].set_color(\"none\")\n",
    "ax.spines[\"top\"].set_color(\"none\")\n",
    "\n",
    "ax.set_ylabel(r\"Computation time $[\\frac{s}{epoch}]$\", fontsize=12)\n",
    "ax.set_xlabel(\"Number of lyapunov exponent\", fontsize=12)\n",
    "\n",
    "experiment_labels = [1, 10, 25, 50, 75, 100]\n",
    "plot_time(tanhpos_n100_weights_N2,experiment_labels,ax,lab=r'Weight, $N^2$')\n",
    "plot_time(tanhpos_n100_gains_shifts_2N,experiment_labels,ax,lab=r'Gains-Shifts, $2N$')\n",
    "plot_time(tanhpos_n100_gains_N,experiment_labels,ax,lab=r'Gains, $N$')\n",
    "ax.legend(fontsize=10, frameon=False)\n",
    "plt.savefig('../data/fig/FINAL/3_RD_Fixed_Effi.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
